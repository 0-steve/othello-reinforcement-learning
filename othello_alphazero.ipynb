{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d8326b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from py_games.Arena import Arena\n",
    "from py_games.OthelloGame import OthelloGame\n",
    "from py_games.OthelloPlayers import *\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb9df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the policy network \n",
    "    \"\"\"\n",
    "    def __init__(self, game):\n",
    "        super().__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.board_x, self.board_y = game.getBoardSize()\n",
    "        self.action_size = game.getActionSize()\n",
    "        self.num_channels = 256  # number of channels for the Conv2d layer\n",
    "        self.dropout = 0.3  # Dropout probability\n",
    "        \n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, self.num_channels, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(self.num_channels, self.num_channels, 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(self.num_channels, self.num_channels, 3, stride=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(self.num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(self.num_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(self.num_channels)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.num_channels*(self.board_x-2)*(self.board_y-2), 512)\n",
    "        self.fc_bn1 = nn.BatchNorm1d(512)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, self.action_size)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s: board configurtion, torch.Tensor with shape (batch_size, board_x, board_y)\n",
    "        Returns:\n",
    "            pi: log probability of actions in state s, torch.Tensor with shape (batch_size, action_size)\n",
    "            v: value of state s, torch.Tensor with shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n",
    "        s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "        s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "        s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
    "        s = s.view(-1, self.num_channels*(self.board_x-2)*(self.board_y-2))\n",
    "\n",
    "        s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.dropout, training=self.training)  # batch_size x 512\n",
    "\n",
    "        # log probability of actions in state s\n",
    "        pi = F.log_softmax(self.fc2(s), dim=1)                                                   # batch_size x action_size\n",
    "        # value of state s\n",
    "        v = torch.tanh(self.fc3(s))                                                              # batch_size x 1\n",
    "\n",
    "        return pi, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4774f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a class to implement MCTS.\n",
    "class MCTS:\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, game, policy_net):\n",
    "        self.game = game\n",
    "        self.policy_net = policy_net\n",
    "        \n",
    "        self.num_MCTS_sims = 50  # number of simulations for MCTS for each action\n",
    "        self.bonus_term_factor = 1.0\n",
    "        \n",
    "        self.Qsa = {}  # stores Q values for s,a\n",
    "        self.Nsa = {}  # stores number of times edge s,a was visited\n",
    "        self.Ns = {}  # stores number of times board s was visited\n",
    "        self.Ps = {}  # stores initial policy (returned by policy network)\n",
    "\n",
    "        self.Es = {}  # stores game.getGameEnded for board s\n",
    "        self.Vs = {}  # stores game.getValidMoves for board s\n",
    "\n",
    "    def getActionProb(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        This function performs num_MCTS_sims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "        \n",
    "        Args:\n",
    "            canonicalBoard: canonical board configuration, a 2D numpy array:\n",
    "                            1=current player, -1=the opponent, 0=empty\n",
    "                            first dim is row , second is column\n",
    "        Returns:\n",
    "            probs: a list with len=action_size, which is a policy vector \n",
    "                   where the probability of the ith action is proportional to Nsa[(s,a)]\n",
    "        \"\"\"\n",
    "        # Doing self.num_MCTS_sims times of simulations starting from the state 'canonicalBoard'\n",
    "        for i in range(self.num_MCTS_sims):\n",
    "            self.search(canonicalBoard)\n",
    "\n",
    "        # Use string representation for the state\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        \"\"\"\n",
    "        Please complete the codes for calculating the updated policy vector 'probs' using 'self.Nsa'\n",
    "        Some information you may need:\n",
    "            self.Nsa[(s, a)] stores number of times edge s,a was visited.\n",
    "            If (s,a) is not in self.Nsa, then s has not been visited.\n",
    "            self.game.getActionSize() returns the number of actions, i.e., n*n+1.\n",
    "        \"\"\"\n",
    "\n",
    "        counts = []\n",
    "        for a in range(self.game.getActionSize()): # self.game.getActionSize() = 26\n",
    "            if (s, a) in self.Nsa:\n",
    "                counts.append(self.Nsa[(s, a)])\n",
    "            else:\n",
    "                counts.append(0)\n",
    "        counts_sum = float(sum(counts))\n",
    "        \n",
    "        probs = []\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if (s, a) in self.Nsa:\n",
    "                prob = self.Nsa[(s, a)]/counts_sum\n",
    "                probs.append(prob)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        This function performs one simulation of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propagated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        \n",
    "        This is a recursive function.\n",
    "        \n",
    "        Args:\n",
    "            canonicalBoard: canonical board configuration, a 2D numpy array:\n",
    "                            1=current player, -1=the opponent, 0=empty\n",
    "                            first dim is row , second is column\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use string representation for the state\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        \n",
    "        # Update self.Es\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        \n",
    "        \n",
    "        if self.Es[s] != 0:  # The game ended, which means that s is a terminal node\n",
    "            # If the current player won, then return -1 (The value for the other player).\n",
    "            # Otherwise, return 1 (The value for the other player).\n",
    "            return -self.Es[s]\n",
    "\n",
    "        if s not in self.Ps:  # There is no policy for the current state s, which means that s is a leaf node (a new state)\n",
    "            \n",
    "            # Set Q(s,a)=0 and N(s,a)=0 for all a\n",
    "            for a in range(self.game.getActionSize()):\n",
    "                self.Qsa[(s, a)] = 0\n",
    "                self.Nsa[(s, a)] = 0\n",
    "            \n",
    "            # Calculate the output of the policy network, which are the policy and the value for state s\n",
    "            board = torch.FloatTensor(canonicalBoard.astype(np.float64)).view(1, self.policy_net.board_x,\n",
    "                                                                              self.policy_net.board_y)\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad():\n",
    "                pi, v = self.policy_net(board)\n",
    "            self.Ps[s] = torch.exp(pi).data.cpu().numpy()[0]  # The policy for state s\n",
    "            v = v.data.cpu().numpy()[0][0]  # The value of state s\n",
    "            \n",
    "            # Masking invalid moves\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s] * valids  \n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "            \n",
    "            self.Vs[s] = valids  # Stores the valid moves\n",
    "            self.Ns[s] = 0\n",
    "            return -v\n",
    "        \n",
    "        # pick the action with the highest upper confidence bound (ucb) and assign it to best_act\n",
    "        best_act = -1\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                \"\"\"\n",
    "                self.Qsa[(s, a)] stores the Q value for s,a\n",
    "                self.bonus_term_factor=1.0 is the factor \"h\" in the UCB\n",
    "                self.Ps stores the policy returned by policy network\n",
    "                self.Ps[s][a] is the probability corresponding to state s and action a\n",
    "                self.Ns[s] stores the number of times board s was visited\n",
    "                self.Nsa[(s, a)] stores number of times edge s,a was visited\n",
    "                \"\"\"\n",
    "\n",
    "                ucb = self.Qsa[(s, a)] + self.bonus_term_factor*self.Ps[s][a]*math.sqrt(self.Ns[s])/(1+self.Nsa[(s, a)])\n",
    "\n",
    "                if ucb > cur_best:\n",
    "                    cur_best = ucb\n",
    "                    best_act = a\n",
    "        \n",
    "        # Continue the simulation: take action best_act in the simulation\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)  # This returns the value for the current player\n",
    "        \n",
    "        \"\"\"\n",
    "        self.Qsa[(s, a)] stores the Q value for s,a\n",
    "        self.Ns[s] stores the number of times board s was visited\n",
    "        self.Nsa[(s, a)] stores number of times edge s,a was visited\n",
    "        v is the value for the current player\n",
    "        \"\"\"\n",
    "        self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "        self.Nsa[(s, a)] = self.Nsa[(s, a)] + 1\n",
    "        \n",
    "        # Update the number of times that s has been visited\n",
    "        self.Ns[s] += 1\n",
    "        \n",
    "        return -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bdf2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a class to implement the whole learning process.\n",
    "class Coach():\n",
    "    \"\"\"\n",
    "    This class executes the self-play + learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "        self.nnet = PolicyNet(game)\n",
    "        self.pnet = PolicyNet(game)  # the competitor network\n",
    "        self.mcts = MCTS(game, self.nnet)\n",
    "        self.epochs = 10  # number of training epochs for each iteration\n",
    "        self.learning_rate = 0.0001\n",
    "        self.batch_size = 64  # batcself.mctsh size\n",
    "        self.trainExamples = []  # historical examples for training\n",
    "        self.numIters = 2  # number of iterations\n",
    "        self.numEps = 20  # number of complete self-play games for one iteration.\n",
    "        self.arenaCompare = 40  # number of games to play during arena play to determine if new net will be accepted.\n",
    "        self.updateThreshold = 0.6  # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Performs numIters iterations with numEps episodes of self-play in each\n",
    "        iteration. After every iteration, it retrains neural network with\n",
    "        examples in trainExamples (which has a maximum length of maxlenofQueue).\n",
    "        It then pits the new neural network against the old one and accepts it\n",
    "        only if it wins >= updateThreshold fraction of games.\n",
    "        \"\"\"\n",
    "        for i in range(1, self.numIters + 1):\n",
    "            print(f'Starting Iter #{i} ...')\n",
    "\n",
    "            for _ in tqdm(range(self.numEps), desc=\"Self Play\"):\n",
    "                self.mcts = MCTS(self.game, self.nnet)  # reset search tree\n",
    "                self.trainExamples.extend(self.executeEpisode()) # save the iteration examples to the history\n",
    "            \n",
    "            # shuffle examples before training           \n",
    "            shuffle(self.trainExamples)\n",
    "\n",
    "            # training new network, keeping a copy of the old one\n",
    "            self.pnet.load_state_dict(self.nnet.state_dict())\n",
    "\n",
    "            optimizer = optim.Adam(self.nnet.parameters(), lr=self.learning_rate)\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                print('EPOCH ::: ' + str(epoch + 1))\n",
    "                self.nnet.train()\n",
    "                \n",
    "                \"\"\"\n",
    "                self.trainExamples is a list that stores historical examples for training\n",
    "                self.trainExamples[i] has the form (canonicalBoard, pi, v)\n",
    "                The output of self.nnet include pi and v, where\n",
    "                    pi are the log probabilities of actions in state s;\n",
    "                    v is the value of state s.\n",
    "                \"\"\"\n",
    "                batch_count = int(len(self.trainExamples) / self.batch_size)\n",
    "                t = tqdm(range(batch_count), desc='Training Net')\n",
    "                sum_l_pi = 0.0  # recording the loss for the policy\n",
    "                sum_l_l_v = 0.0  # recording the loss for the value\n",
    "                count_l = 0\n",
    "                for _ in t:\n",
    "                    sample_ids = np.random.randint(len(self.trainExamples), size=self.batch_size)\n",
    "                    boards, pis, vs = list(zip(*[self.trainExamples[i] for i in sample_ids]))\n",
    "                    boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "                    target_pis = torch.FloatTensor(np.array(pis))\n",
    "                    target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "                    # compute output\n",
    "                    out_pi, out_v = self.nnet(boards)\n",
    "\n",
    "                    l_pi = np.mean(np.inner(list(self.mcts.Ps.values())[i], out_pi.detach())) / batch_count\n",
    "                    l_pi = -l_pi\n",
    "                    \n",
    "                    l_v = torch.nn.MSELoss() # loss function from pytorch\n",
    "                    l_v = l_v(out_v, target_vs.detach()) / batch_count\n",
    "                       \n",
    "                    total_loss = l_pi + l_v\n",
    "\n",
    "                    # record loss\n",
    "                    sum_l_pi += l_pi.item()\n",
    "                    sum_l_l_v += l_v.item()\n",
    "                    count_l += 1\n",
    "                    t.set_postfix(Loss_pi=f'{sum_l_pi / count_l:.2e}', Loss_v=f'{sum_l_l_v / count_l:.2e}')\n",
    "                \n",
    "                    # compute gradient and do SGD step, using ``optimizer''\n",
    "                    optimizer.zero_grad() # zero out current gradients in network\n",
    "                    total_loss.backward() # back propagate network's gradient\n",
    "                    optimizer.step() # optimize step \n",
    "            \n",
    "            pmcts = MCTS(self.game, self.pnet)\n",
    "            nmcts = MCTS(self.game, self.nnet)\n",
    "\n",
    "            print('PITTING AGAINST PREVIOUS VERSION')\n",
    "            arena = Arena(lambda x: np.argmax(pmcts.getActionProb(x)),\n",
    "                          lambda x: np.argmax(nmcts.getActionProb(x)), self.game)\n",
    "            pwins, nwins, draws = arena.playGames(self.arenaCompare)\n",
    "\n",
    "            print('NEW/PREV WINS : %d / %d ; DRAWS : %d' % (nwins, pwins, draws))\n",
    "            if pwins + nwins == 0 or float(nwins) / (pwins + nwins) < self.updateThreshold:\n",
    "                print('REJECTING NEW MODEL')\n",
    "                self.nnet.load_state_dict(self.pnet.state_dict())\n",
    "            else:\n",
    "                print('ACCEPTING NEW MODEL')\n",
    "                self.pnet.load_state_dict(self.nnet.state_dict())\n",
    "                self.trainExamples = []\n",
    "        \n",
    "    def play(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            canonicalBoard: canonical board configuration, a 2D numpy array:\n",
    "                            1=current player, -1=the opponent, 0=empty\n",
    "                            first dim is row , second is column\n",
    "        Returns:\n",
    "            action: Putting a disc on row x and column y of the board corresponds to action=x*n+y. action=n*n means passing.\n",
    "            (Row and column are counting from 0 to n-1.) \n",
    "        \"\"\"\n",
    "        mcts = MCTS(self.game, self.nnet)\n",
    "        action = np.argmax(mcts.getActionProb(canonicalBoard))\n",
    "        return action\n",
    "    \n",
    "    def executeEpisode(self):\n",
    "        \"\"\"\n",
    "        This function executes one episode of self-play, starting with player 1 (Black player).\n",
    "        As the game is played, each turn is added as a training example to\n",
    "        trainExamples. The game is played till the game ends. After the game\n",
    "        ends, the outcome of the game is used to assign values to each example\n",
    "        in trainExamples.\n",
    "\n",
    "        Returns:\n",
    "            trainExamples: a list of examples of the form (canonicalBoard, pi, v)\n",
    "                           pi is the MCTS informed policy vector, v is +1 if\n",
    "                           the player eventually won the game, -1 if the player lost the game, and otherwise 0.000001\n",
    "        \"\"\"\n",
    "        trainExamples = []\n",
    "        board = self.game.getInitBoard()\n",
    "        self.curPlayer = 1\n",
    "        episodeStep = 0\n",
    "\n",
    "        while True:\n",
    "            episodeStep += 1\n",
    "            canonicalBoard = self.game.getCanonicalForm(board, self.curPlayer)\n",
    "            \n",
    "            # After 10 steps, we use the greedy action rather than a random action\n",
    "            if episodeStep < 10:\n",
    "                pi = self.mcts.getActionProb(canonicalBoard)\n",
    "            else:\n",
    "                pi = list(np.zeros((self.game.getActionSize(),)))\n",
    "                pi[np.argmax(self.mcts.getActionProb(canonicalBoard))] = 1\n",
    "            \n",
    "            # Add symmetric samples\n",
    "            sym = self.game.getSymmetries(canonicalBoard, pi)\n",
    "            \n",
    "            for b, p in sym:\n",
    "                trainExamples.append([b, self.curPlayer, p, None])\n",
    "            \n",
    "            # Take action according to the policy pi\n",
    "            action = np.random.choice(len(pi), p=pi)\n",
    "            board, self.curPlayer = self.game.getNextState(board, self.curPlayer, action)\n",
    "\n",
    "            r = self.game.getGameEnded(board, self.curPlayer)\n",
    "\n",
    "            if r != 0:  # if the current episode of game ended\n",
    "                trainExamples = [(x[0], x[2], r * ((-1) ** (x[1] != self.curPlayer))) for x in trainExamples]\n",
    "                return trainExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc32930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Iter #1 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6a4dd6663643d792cb3951ec53bf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Self Play:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938fa12dbf48456883fcbe493db10ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe44334e11354fad829234429b98d699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb12583f58a4877a5cb7bd94fde50be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14dc01a62314474aa3345063a5f57580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf423a3bad504be59c7697daae27d274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8949c5b2469c4d37a0fb40bfba505718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9de5630bfb4cea891ffe6ebdc0b227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4aa2aaf44e04d8a8f24e9360dc265e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c417c8f2d2a54a84992d9ad815496aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdd0a27ff3f4207bb9fc0d21add9b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PITTING AGAINST PREVIOUS VERSION\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3dfe99b624495d835c51dfba35f78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Arena.playGames (1):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec6a0279e3c4b2fa4e90432bd89cc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Arena.playGames (2):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW/PREV WINS : 13 / 27 ; DRAWS : 0\n",
      "REJECTING NEW MODEL\n",
      "Starting Iter #2 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a279ecdf1974f36a249dc4234bfdc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Self Play:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c677da7182498cbfb4d469d3512976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270f714586f345508b770dbf4499a449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c314656ddfde4e8798114aa4e245fa44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac317a28d417466bb202e87bf0f47a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f966360db06f4fb8ae92d4436e3a5c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c771854ac4d64dc6a546b189fada17c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3222c8dd70420db8832daf9d208a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62ebf1743d54347a9e3b27bfc8f602b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24396502a724b439a1b70c6e6b92faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ::: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5913c9a9bce148cab0e5868c065c8a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Net:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PITTING AGAINST PREVIOUS VERSION\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0d64485b45416e8754f382ada08f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Arena.playGames (1):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b46fafe7e8494dabd16d6322178440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Arena.playGames (2):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW/PREV WINS : 11 / 29 ; DRAWS : 0\n",
      "REJECTING NEW MODEL\n",
      "\n",
      "TESTING\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b717fbc27f54da2a5a1c9e567b7779b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Arena.playGames (1):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ed7aa6290b488e96a280a42c43eb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Arena.playGames (2):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractin won:  0.61\n"
     ]
    }
   ],
   "source": [
    "# First call the training agemt, ``coach.train()``,\n",
    "# Then test your agent by pitting it against a random player for 100 games.\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "game = OthelloGame(5)  # An Othello game with a 5*5 board\n",
    "random_player = RandomPlayer(game).play\n",
    "coach = Coach(game)\n",
    "coach.train()\n",
    "print(\"\\nTESTING\")\n",
    "arena = Arena(coach.play, random_player, game)\n",
    "oneWon, twoWon, draws = arena.playGames(100)\n",
    "fraction_won = oneWon / 100\n",
    "print(\"Fractin won: \", fraction_won)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe6c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
